//Script GUID:de366ac5-fa6e-4538-9133-092819d4e338
//Used for tracking history

/*
 * This script basically exists to address an issue in Kusto. Although purported to
 * be able to handle wide rows, Kusto in fact fails to properly vectorize commonly-
 * used operations (so if, for example, you were to want to sum your rowset column-
 * by-column, most databases would handle this by distributing rows by an aggregand
 * and then doing vectorized summation on the column data. Kusto doesn't handle the
 * operation sensibly, instead blowing up in terms of computation time due (afaict)
 * to attempting to split the column stores data, and then handle the generation of
 * each column separately with its own operator. This means a query which should in
 * most databases take milliseconds takes tens of minutes in Kusto.) This query, to
 * reduce the row size, pivots our data heavily down to 6 columns: domain, appname,
 * numpoints, metric, instances, and numtraces. This should work significantly more
 * performantly downstream.
 */

// Some necessary C# code
#CS
using Microsoft.SCOPE.Types;
using System;
using System.Collections.Generic;
using System.IO;
using System.Text;
using System.Text.RegularExpressions;
using ScopeRuntime;

public class UnpivotProcessor : Processor
{
    private static readonly string telemetryConfig = @"
<<<HEADER_FILE>>>
        ";
    public override bool ConstantResultSchema
    {
        get
        {
            return true;
        } 
    }
    public override bool RowLevelProcessor
    {
        get
        {
            return true;
        }
    }
    private List<Tuple<String, String, String, String>> stuff = new List<Tuple<String, String, String, String>>();
    private bool hasInitialized = false;
    private void InternalInitialize()
    {
       if(!hasInitialized)
        {
            hasInitialized = true;
            Regex nocomments = new Regex(@"//[^\n]*\n");
            String uncommented = nocomments.Replace(telemetryConfig, "");
            Regex builtins = new Regex(@"((?<Type>BLOCK_START)\((?<BlockName>[a-zA-Z0-9_]*), *(?<BlockCount>[0-9]*)\))|((?<Type>ENTRY_BUILTIN)\((?<EMCAVersion>[a-zA-Z0-9_]*), *(?<BaseObject>[a-zA-Z0-9_]*), *(?<LinkType>[a-zA-Z0-9_]*), *(?<FunctionName>[a-zA-Z0-9_]*)\))|((?<Type>ENTRY_LANGFEATURE)\((?<EMCAVersion>[a-zA-Z0-9_]*), *(?<PointName>[a-zA-Z0-9_]*)\))|((?<Type>ENTRY_TELPOINT)\((?<PointName>[a-zA-Z0-9_]*)\))");
            foreach ( Match match in builtins.Matches(uncommented) )
            {
                switch (match.Groups["Type"].Value)
                {
                    case "BLOCK_START":
                        break;
                    case "ENTRY_BUILTIN":
                        {
                            string fieldtag = match.Groups["BaseObject"].Value + "_" + match.Groups["LinkType"].Value + "_" + match.Groups["FunctionName"].Value;
                            string fieldtag_traces = fieldtag + "_traces";
                            stuff.Add(new Tuple<String, String, String, String>(fieldtag, fieldtag + "_props_sum", fieldtag + "_callCount_sum", fieldtag_traces));
                            //stuff.Add(new Tuple<string, string, string>(fieldtag + "_callCount", fieldtag + "_callCount_sum", fieldtag_traces));
                            //stuff.Add(new Tuple<string, string, string>(fieldtag + "_dMCallCount", fieldtag + "_dMCallCount_sum", fieldtag_traces));
                        }
                        break;
                    case "ENTRY_LANGFEATURE":
                    case "ENTRY_TELPOINT":
                        {
                            string fieldtag = match.Groups["PointName"].Value;
                            stuff.Add(new Tuple<String, String, String, String>(fieldtag, null, fieldtag + "_sum", fieldtag + "_traces"));
                        }
                        break;
                }
            }
        } 
    }
    public override Schema GetOutputSchemaAtCompileTime(string[] requestedColumns, string[] args, Schema input)
    {
        return new Schema("Domain:string,ApplicationName:string,numpoints:long,metric:string,parseCount:long,runCount:long,traceCount:long");
    }
    public override Schema Produces(string[] requestedColumns, string[] args, Schema input)
    {
        return GetOutputSchemaAtCompileTime(requestedColumns, args, input);
    }
    public override void Initialize(RowSet left, RowSet right, string[] args)
    {
        base.Initialize(left, right, args);
    }
    public override IEnumerable<Row> Process(RowSet input, Row outputRow, string[] args)
    {
        InternalInitialize();
        // We're going to emit a _lot_ of rows here
        foreach (Row row in input.Rows)
        {
            // iterate over the field groupings
            foreach (Tuple<String, String, String, String> tuple in stuff)
            {
                // fill in the ouput row
                row["Domain"].CopyTo(outputRow["Domain"]);
                row["ApplicationName"].CopyTo(outputRow["ApplicationName"]);
                row["numpoints"].CopyTo(outputRow["numpoints"]);
                outputRow["metric"].Set(tuple.Item1);
                if(tuple.Item2 != null)
                {
                    row[tuple.Item2].CopyTo(outputRow["parseCount"]);
                }
                else
                {
                    outputRow["parseCount"].Set(0);
                }
                if (tuple.Item3 != null)
                {
                    row[tuple.Item3].CopyTo(outputRow["runCount"]);
                }
                else
                {
                    outputRow["runCount"].Set(0);
                }
                row[tuple.Item4].CopyTo(outputRow["traceCount"]);
                // return each row
                yield return outputRow;
            }
        }
    }
}
#ENDCS

// hook up to the asimov date inputs, so that back runs work properly
#DECLARE startDate DateTime = IF("@@startDate@@".StartsWith("@@"), DateTime.UtcNow, DateTime.Parse("@@startDate@@"));
#DECLARE streamDate DateTime = @startDate.AddDays(-1);

// The pivoted input file
#IF(LOCAL)
    #DECLARE inputFileName string = @"c:\temp\ESBuiltinsData_Aggregated.ss";
#ELSE
    #DECLARE inputFileName string = string.Format( @"/shares/asimov.prod.data/PublicPartner/Processed/ChakraJavaScript/{0}/{1:yyyy}/{1:MM}/{0}_{1:yyyy}_{1:MM}_{1:dd}.ss", "ESBuiltinsData_Aggregated" , @streamDate );
#ENDIF

// The post-aggregation output file
#IF(LOCAL)
    #DECLARE outputFileName string = @"c:\temp\ESBuiltinsData_Aggregated_Pivoted.ss";
#ELSE
    #DECLARE outputFileName string = string.Format( @"/shares/asimov.prod.data/PublicPartner/Processed/ChakraJavaScript/{0}/{1:yyyy}/{1:MM}/{0}_{1:yyyy}_{1:MM}_{1:dd}.ss", "ESBuiltinsData_Aggregated_Pivoted" , @streamDate );
#ENDIF



// Grab the already-aggregated data from the previous stage's output file.i
inputData = 
    SELECT
        *
    FROM ( SSTREAM @inputFileName );


// Pivot that data down to 5 columns
// Query construction strategy:
//   Custom c# to unpivot table
outputData =
    PROCESS inputData
    PRODUCE *
    USING UnpivotProcessor;


OUTPUT outputData TO SSTREAM @outputFileName WITH STREAMEXPIRY "365";
